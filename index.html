<!-- --> 
<!-- saved from url=(0036)http://appsrv.cse.cuhk.edu.hk/~xchu/ -->
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd"><HTML 
xml:lang="en" xmlns="http://www.w3.org/1999/xhtml"><HEAD><META 
content="IE=11.0000" http-equiv="X-UA-Compatible">

<META name="GENERATOR" content="MSHTML 11.00.9600.18125">
<META http-equiv="Content-Type" content="text/html;charset=utf-8">
<LINK href="style/jemdoc.css" rel="stylesheet" type="text/css">
<TITLE>Haoli Bai (ÊüèÊòäÁ´ã) </TITLE>
</HEAD>

<BODY>
<DIV id="layout-content">
<DIV id="make-narrow">
<DIV id="toptitle">

<H1>Haoli Bai (ÊüèÊòäÁ´ã) </H1></DIV>

<P>
<img src="img/square_light.jpg" align="right" style="width:224px;height:224px;"> 
</P>

<P>Researcher, Huawei Noah's Ark Lab <BR></P>
<P>Hong Kong SAR, China<BR></P>
<P><B>Email</B>: haolibai [at] gmail.com</P>
<!-- <P><B>WeChat</B>: wxidbhl</P> -->
<p> 
<a href="https://scholar.google.com/citations?user=pk7jX3gAAAAJ&hl=zh-CN"><img src="./img/google_scholar.png" height="30px" style="margin-bottom:-3px"></a>
<a href="https://dblp.org/pid/195/9712.html"><img src="./img/dblp.png" height="30px" style="margin-bottom:-3px"></a>
<a href="https://github.com/haolibai"><img src="./img/github_s.jpg" height="30px" style="margin-bottom:-3px"></a>
<a href="https://www.linkedin.com/in/baihaoli-407136142/"><img src="./img/LinkedIn_s.png" height="30px" style="margin-bottom:-3px"></a>
</p>
<!-- <P><A href="assets/cv.pdf">[Curriculum Vitae]</A></P> -->
<!-- <P><A href="https://dblp.uni-trier.de/pers/hd/b/Bai:Haoli">[DBLP]</A> -->
<!-- <A href="https://github.com/haolibai">[GitHub]</A> -->
<!--<a href="http://appsrv.cse.cuhk.edu.hk/~hlbai/CV_Haoli.pdf">[CV]</A>-->
</P>


<H2>General</H2>
<P>I am currently a researcher at Huawei Noah's Ark Lab. I obtained my Ph.D. degree from The Chinese University of Hong Kong in 2021, supervised by <A href="https://www.cse.cuhk.edu.hk/lyu/">Prof. Michael R. Lyu</A> and <A href="https://www.cse.cuhk.edu.hk/irwin.king/">Prof. Irwin King</A>. Prior to that, I received the B.Eng. Degree from Yingcai Honors College of University of Electronic Science and Technology of China in 2017. <BR>
<BR>

<B>[Hiring]</B> I am looking for research/engineering interns with strong machine learning and natural language processing background. Please drop me an email if you are interested. Base: HK or Shenzhen. 

<H2>Research Topics</H2>
<P>Acceleration of Large Language Models, Multi-modal Pre-training</P>


<H2>News</H2>

  <UL>
  <LI>
  [2025-4] I will serve as the Area Chair for NeurIPS 2025.
  </LI>
</UL>
  
  <UL>
  <LI>
  [2025-4]üî•Our preprint <A href="https://arxiv.org/abs/2504.04823v1"> "Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models" </A> is now released and trending on <A href="https://www.alphaxiv.org/abs/2504.04823">alphaXiv</A>. Code will be available <A href="https://github.com/ruikangliu/Quantized-Reasoning-Models"> here</A>.
  </LI>
</UL>
  
  <UL>
  <LI>
  [2024-10]üî•Our preprint <A href="https://arxiv.org/pdf/2410.09426"> "FlatQuant: Flatness Matters for LLM Quantization" </A> sets up new records for LLM quantization! Code is available <A href="https://github.com/ruikangliu/FlatQuant"> here</A>.
  </LI>
</UL>
  
  <UL>
  <LI>
  [2024-3] Our work <A href="https://openreview.net/forum?id=3wReeptY6X"> "Visually Guided Generative Text-Layout Pre-training for Document Intelligence" </A> is accepted by NACCL 2024. 
  </LI>
</UL>
  
  <UL>
  <LI>
  [2024-3] Our work <A href="https://arxiv.org/abs/2403.01241"> "IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact" </A>, an orthogonal approach to enhance existing quantized LLMs, is accepted by ACL 2024 Findings.
  </LI>
</UL>

<!--   <UL>
  <LI>
  [2024-3] Our work <A href="https://arxiv.org/abs/2403.07839v1"> "MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric" </A> is accepted by CVPR 2024.
  </LI>
</UL> -->
  
<!--   
<UL>
  <LI>
  [2024-1] Our work <A href="https://openreview.net/forum?id=Tr0lPx9woF"> "Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models" </A> is accepted by ICLR 2024.
  </LI>
</UL> -->
  
<!--  <UL>
  <LI>
  [2023-5] Two papers (one main conference and one finding) were accepted by ACL 2023, congratulations!
  </LI>
</UL>
  
<!--   <UL>
  <LI>
  [2022-12] Our recent work <A href="https://arxiv.org/abs/2212.09621"> "Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding" </A> is available.
  </LI>
</UL> -->
  
<!--   <UL>
  <LI>
  [2022-9-15] Our paper <A href="https://arxiv.org/abs/2109.15082"> "Towards Efficient Post-training Quantization for Pre-trained Language Models" </A> was accepted by NeurIPS 2022.
  </LI>
</UL> -->

<!--   <UL>
  <LI>
  [2021-9-1] I passed my Ph.D. oral defense! Many thanks to my supervisors and committees.
  </LI>
</UL>
 -->
<!-- <UL>
<UL>
  <LI>
  [2021-5-6] Our paper <A href="https://arxiv.org/abs/2012.15701"> "BinaryBERT: Pushing the Limit of BERT Quantization" </A> was accepted by ACL 2021 with <B>5 5 4</B>. The code is available at <A href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT"> here </A>.
  </LI>
</UL>
-->
  
<!-- <UL>
  <LI>
  [2021-5-1] Our paper <A href="https://arxiv.org/abs/2004.09764"> Discrete Auto-regressive Variational Attention Models for Text Modeling </A> was accepted by IJCNN 2021 with oral presentation.
  </LI>
</UL>
 -->
  
<!-- <UL>
  <LI>
  [2021-1-1] Our preprint <A href="https://arxiv.org/abs/2012.15701"> BinaryBERT: Pushing the Limit of BERT Quantization </A> was released. 
  </LI>
</UL>
 -->
  
<!-- <UL>
  <LI>
  [2020-9-26] Our paper <A href="https://proceedings.neurips.cc/paper/2020/file/42cd63cb189c30ed03e42ce2c069566c-Paper.pdf"> Revisiting Parameter Sharing for Automatic Neural Channel Number Search </A> was accepted by NeurIPS 2020. 
  </LI>
</UL> -->

<!-- <UL>
  <LI>
  [2020-7-1] I joined Huawei Noah's Ark Lab as a research intern. 
  </LI>
</UL> -->

<H2> Selected Research </H2>
&nbsp;&nbsp;&nbsp;&nbsp;(*: Equal contribution; #: Corresponding author; +: Mentor.)

<UL>
  <LI>
  <P> Ruikang Liu<sup>*</sup>, Yuxuan Sun<sup>*</sup>, Manyi Zhang<sup>*</sup>, <B>Haoli Bai</B><sup>#+</sup>, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou<sup>#</sup><BR>
    <A href="https://arxiv.org/abs/2504.04823v1"> Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</A><BR>
    <I> Preprint arXiv:2504.04823, 2025 </I>
    <A href="https://github.com/ruikangliu/Quantized-Reasoning-Models">[Code]</A>
   </P>
  </LI>
</UL>
  
<UL>
  <LI>
  <P> Yuxuan Sun<sup>*</sup>, Ruikang Liu<sup>*</sup>, <B>Haoli Bai</B><sup>#+</sup>, Han Bao, Kang Zhao, Yuening Li, Jiaxin Hu, Xianzhi Yu, Lu Hou, Chun Yuan, Xin Jiang, Wulong Liu, Jun Yao <BR>
    <A href="https://arxiv.org/pdf/2410.09426"> FlatQuant: Flatness Matters for LLM Quantization</A><BR>
    <I> Preprint arXiv:2410.09426, 2024 </I>
    <A href="https://github.com/ruikangliu/FlatQuant">[Code]</A>
   </P>
  </LI>
</UL>
  
<UL>
  <LI>
  <P> Zhiming Mao, <B>Haoli Bai</B><sup>#+</sup>, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong <BR>    
    <A href="https://aclanthology.org/2024.naacl-long.264.pdf"> Visually Guided Generative Text-Layout Pre-training for Document Intelligence </A><BR>
    <I> NACCL'24, the North American Chapter of the Association for Computational Linguistics, 2024 </I>
    <A href="https://github.com/Veason-silverbullet/ViTLP">[Code]</A>
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P> Ruikang Liu, <B>Haoli Bai</B><sup>+</sup>, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan <BR>    
    <A href="https://arxiv.org/abs/2403.01241"> IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact </A><BR>
    <I> ACL 2024, Findings </I>
    <A href="https://github.com/ruikangliu/IntactKV">[Code]</A>
   </P>
  </LI>
</UL>
  
<UL>
  
  <LI>
  <P> Haokun Lin, <B>Haoli Bai</B><sup>+</sup>, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, Zhenan Sun <BR>    
    <A href="https://arxiv.org/abs/2403.07839v1"> MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric </A><BR>
    <I> CVPR'24: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024 </I>
   </P>
  </LI>
</UL>
  
<UL>
  
  <LI>
  <P> Yingtao Zhang, <B>Haoli Bai</B><sup>+</sup>, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci <BR>    
    <A href="https://openreview.net/forum?id=Tr0lPx9woF"> Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models </A><BR>
    <I> ICLR'24: The Twelfth International Conference on Learning Representations, 2024 </I>
    <A href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">[Code]</A>
   </P>
  </LI>
</UL>
  
<UL>
  <LI>
  <P><B>Haoli Bai</B><sup>*</sup>, Zhiguang Liu<sup>*</sup>, Xiaojun Meng<sup>*</sup>, Wentao Li, Shuang Liu, Nian Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu <BR>    
    <A href="https://arxiv.org/abs/2212.09621">Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding</A><BR>
    <I> ACL'23: The 61th Annual Meeting of the Association for Computational Linguistics, 2023 </I>
   </P>
  </LI>
</UL>

<UL>
  <LI>
    <P>Chaofan Tao, Lu Hou<sup>+</sup>, <B>Haoli Bai</B><sup>+</sup>, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong <BR>    
    <A>Structured Pruning for Efficient Generative Pre-trained Language Models</A><BR>
    <I> ACL'23 Findings: The 61th Annual Meeting of the Association for Computational Linguistics, 2023 </I>
   </P>
  </LI>
</UL>  
    
<UL>
  <LI>
  <P><B>Haoli Bai</B>, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, Michael Lyu<BR>    
    <A href="https://arxiv.org/pdf/2109.15082.pdf">Towards Efficient Post-training Quantization of Pre-trained Language Models</A><BR>
    <I>NeurIPS'22: Proceedings of the 36th conference on Neural Information Processing Systems</I>, 2022.
  </P>
  </LI>
</UL>
  
<UL>
  <LI>
  <P><B>Haoli Bai</B>, Hongda Mao, Dinesh Nair<BR>
    <A href="https://arxiv.org/abs/2012.15701"> Dynamically pruning segformer for efficient semantic segmentation </A><BR>
    <I>ICASSP'22: IEEE International Conference on Acoustics, Speech and Signal Processing, 2022.</I>
   </P>
  </LI>
</UL>
    
<UL>
  <LI>
  <P><B>Haoli Bai</B>, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, Irwin King <BR>
    <A href="https://arxiv.org/abs/2012.15701"> BinaryBERT: Pushing the Limit of BERT Quantization </A><BR>
    <I>ACL'21: The 59th Annual Meeting of the Association for Computational Linguistics, 2021.</I> <B> Accepted with scores 5, 5, 4.</B>
    <A href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT">[Code]</A>
   </P>
  </LI>
</UL>

<!-- <UL>
  <LI>
  <P>Xianghong Fang<sup>*</sup>, <B>Haoli Bai</B><sup>*</sup>, Jian Li, Zenglin Xu, Michael Lyu, Irwin King <BR>
    <A href="https://arxiv.org/abs/2004.09764"> Discrete Auto-regressive Variational Attention Models for Text Modeling </A><BR>
    <I>IJCNN'21: International Joint Conference on Neural Networks, 2021. Oral presentation.</I>
    <A href="https://github.com/sunset-clouds/DAVAM">[Code]</A>
   </P>
  </LI>
</UL>     -->
    
<UL>
  <LI>
  <P><B>Haoli Bai</B><sup>*</sup>, Jiaxing Wang<sup>*</sup>, Jiaxiang Wu, Xupeng Shi, Junzhou Huang, Irwin King, Michael Lyu, Jian Cheng <BR>
    <A href="https://proceedings.neurips.cc/paper/2020/file/42cd63cb189c30ed03e42ce2c069566c-Paper.pdf"> Revisiting Parameter Sharing for Automatic Neural Channel Number Search </A><BR>
    <I>NeurIPS'20: Proceedings of the 34th conference on Neural Information Processing Systems</I>, 2020.
    <A href="https://github.com/haolibai/APS-channel-search">[Code]</A>
   </P>
  </LI>
</UL>

<!-- <UL>
  <LI>
  <P>Kuo Zhong*, Yin Wei*, Chun Yuan, <B>Haoli Bai</B>, Junzhou Huang<BR>
    <A href="https://dl.acm.org/doi/pdf/10.1145/3394486.3403079">TranSlider: Transfer Ensemble Learning from Exploitation to Exploration</A><BR>
    <I>KDD'20: Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</I>, 2020.
   </P>
  </LI>
</UL>  -->
 
<!-- <UL>
  <LI>
  <P>Jiaxing Wang, <B>Haoli Bai</B>, Jiaxiang Wu, Jian, Cheng<BR>
    <A href="https://ieeexplore.ieee.org/abstract/document/9018278">Bayesian Automatic Model Compression</A><BR>
    <I>IEEE Journal of Selected Topics in Signal Processing</I>, 2020.
   </P>
  </LI>
</UL>
-->
  
<UL>
  <LI>
  <P><B>Haoli Bai</B>, Jiaxiang Wu, Irwin King, Michael Lyu<BR>
    <A href="https://arxiv.org/abs/1911.09450">Few Shot Network Compression via Cross Distillation</A><BR>
    <I>AAAI'20: Proceedings of the 34th AAAI Conference on Artificial Intelligence</I>, 2020.
    <A href="https://github.com/haolibai/Cross-Distillation">[Code]</A>
    <A href="assets/aaai20_poster.pdf">[Poster]</A>
   </P>
  </LI>
</UL>

<!-- <UL>
  <LI>
  <P>Jiaxing Wang, Jiaxiang Wu, <B>Haoli Bai</B>, Jian Cheng <BR>
    <A href="">M-NAS: Meta Neural Architecture Search</A><BR>
    <I>AAAI'20: Proceedings of the 34th AAAI Conference on Artificial Intelligence</I>, 2020.
   </P>
  </LI>
</UL>

<UL>
  <LI>
  <P>Yuhang Li, Dong Xin, Saiqian Zhang, <B>Haoli Bai</B>, Yuanpeng Chen, Wei Wang <BR>
    <A href="https://arxiv.org/abs/1912.02057">RTN: Reparameterized Ternary Network</A><BR>
    <I>AAAI'20: Proceedings of the 34th AAAI Conference on Artificial Intelligence</I>, 2020.
   </P>
  </LI>
</UL>
-->
  
<!-- 
<UL>
  <LI>
  <P>Liangjian Wen, Xuanyang Zhang, <B>Haoli Bai</B>, Zenglin Xu <BR>
    <A href="https://www.sciencedirect.com/science/article/pii/S0893608019303776?dgcid=coauthor">Structured Pruning of Recurrent Neural Networks through Neuron Selection</A><BR>
    <I>Neural Networks</I>, Volume 123, Pages 134-141, 2020.
   </P>
  </LI>
</UL> -->

    
<!-- <UL>
  <LI>
  <P>Jiaxiang Wu, Yao Zhang, <B>Haoli Bai</B>, Huasong Zhong, Jinlong Hou, Wei Liu, Wenbing Huang, Junzhou Huang<BR>
    <A href="https://openreview.net/forum?id=H1fWoYhdim">PocketFlow: An Automated Framework for Compressing and Accelerating Deep Neural Networks</A><BR>
    <I>NIPS 2018 workshop on Compact Deep Neural Networks with industrial applications</I>, 2018.
   </P>
  </LI>
</UL> -->

<UL>
  <LI>
  <P><B>Haoli Bai</B>, Zhuangbin Chen, Michael Lyu, Irwin King, Zenglin Xu<BR>    
    <A href="https://17a11ed1-a-62cb3a1a-s-sites.googlegroups.com/site/nipsts2017/NIPS_2017_TSW_paper_19.pdf?attachauth=ANoY7cpYZEIRsiSfABAUrEA9qufYoQHayE6Tig80lyv_oBnhCE3RxDLpBtUn_N-sHv7XHb4dtMJ_L7menG9o0wrZBg9UABgZW9hQ2o57fv1yJqt3rLt-iC-AlKGRdb0c3Nszm2JRK_0-aXlpM8YvxqLUz89WKshOLN8djDBd81N9y5xaSrZxPDmGUh0pot2_HnguXwBAY2vT-HAj_kKRpJeEZx6V8HTvyjns_W8G3Phf8gt_tVFs1SY%3D&attredirects=0">Neural Relational Topic Models for Scientific Article Analysis</A><BR>
    <I>CIKM'18: Proceedings of The 27th International Conference on Information and Knowledge Management</I>, 2018.
    <A href="https://github.com/zbchern/Neural-Relational-Topic-Models">[Code]</A>
   </P>
  </LI>
</UL>

<!-- <UL>
  <LI>
  <P>Hao Liu, Lirong He, <B>Haoli Bai</B>, Bo Dai, Zenglin Xu, Kun Bai<BR>    
    <A href="https://17a11ed1-a-62cb3a1a-s-sites.googlegroups.com/site/nipsts2017/NIPS_2017_TSW_paper_19.pdf?attachauth=ANoY7cpYZEIRsiSfABAUrEA9qufYoQHayE6Tig80lyv_oBnhCE3RxDLpBtUn_N-sHv7XHb4dtMJ_L7menG9o0wrZBg9UABgZW9hQ2o57fv1yJqt3rLt-iC-AlKGRdb0c3Nszm2JRK_0-aXlpM8YvxqLUz89WKshOLN8djDBd81N9y5xaSrZxPDmGUh0pot2_HnguXwBAY2vT-HAj_kKRpJeEZx6V8HTvyjns_W8G3Phf8gt_tVFs1SY%3D&attredirects=0">Structured Inference for Recurrent Hidden Semi-Markov Model</A><BR>
    <I>IJCAI'18: Proceedings of The 27th International Joint Conference on Artificial Intelligence</I>, 2018.
   </P>
  </LI>
</UL> -->

<!--
<UL>
  <LI>
  <P>Hao Liu, <B>Haoli Bai</B>, Lirong He, Zenglin Xu<BR>    
    <A href="http://roseyu.com/time-series-workshop/supmissions/TSW2017_paper_17.pdf">Stochastic Sequential Neural Networks with Structured Inference</A><BR>
    <I>Time Series Workshop on International Conference on Machine Learning</I>, 2017.
   </P>
  </LI>
</UL>
-->

<!-- <UL>
  <LI>
  <P>Bin Liu, Zenglin Xu, Bo Dai, <B>Haoli Bai</B>, Xianghong Fang, Yazhou Ren, Shandian Zhe<BR>    
    <A href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7966296">Learning from Semantically Dependent Multi-Tasks</A><BR>
    <I>IJCNN'17: Proceedings of The 30th International Joint Conference on Neural Networks</I>, 2017.    
   </P>
  </LI>
</UL> -->

<UL>
  <LI>
  <P><B>Haoli Bai</B>, Zenglin Xu, Bin Liu, Yingming Li<BR>    
    <A href="http://proceedings.mlr.press/v63/bai103.html">Hierarchical Probabilistic Matrix Factorization with Network Topology for Multi-relational Social Network</A><BR>
    <I>ACML'16: Proceedings of The 8th Asian Conference on Machine Learning</I>, 2016, <B>Best Student Paper Runner-up</B>.    
   </P>
  </LI>
</UL>

<!-- <H2>Preprints</H2> -->


<H2>Projects</H2>
<UL>
  <LI>
  <div style="float:left;"> <B>PocketFlow</B>: An Automated Framework for Compressing and Accelerating DNNs <div style="float:right;"><A href="https://github.com/tencent/PocketFlow"> [Code]</A><A href="https://pocketflow.github.io/"> [Doc]</A> </div>
  <P>
  PocketFlow automatically searches for optimal model compression strategies such as network pruning, quantization, knowledge distillation with little human efforts, and also supports TFLite deployment on Andriod devices. It has collected 2600+ stars and 480+ folks. </P><BR>
    <!-- I am responsible for developing the quantization modules and the associated hyper-parameter optimizer with the RL engine. 
    Our 8-bit quantized MobileNet-V2 achieves around 3.0x speed-up deployed by TF-Lite, with no performance drop~(Top-1 Acc. 72.26%) on ImageNet.<BR> -->
  </LI>
</UL>

   
<H2>Services</H2>
  
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"><B>Area Chair</B>: NeurIPS 2025</div>
  </div>
  
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"><B>Conference Senior PC Member</B>: IJCAI 2021</div>
  </div>
  
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"><B>Conference PC Member</B>: ICLR 22-25, ICML 21-25, NeurIPS 20-24, ACL ARR 25, COLM 25, ICCV 25, AAAI 19-21, IJCAI 20</div>
  </div>
  
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;"><B>Journal Reviewer</B>: Cognitive Computation, Neural Networks, Neurocomputing</div>
  </div>

<H2>Selected Awards</H2>
    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">Excellent Intern</div>
     <div style="float:right;">Huawei Noah's Ark Lab, 2021</div>
    </div>
    
    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">AAAI Student Travel Grant</div>
     <div style="float:right;">AAAI 2020</div>
    </div>
   
    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">ACM Student Travel Grant</div>
     <div style="float:right;">CIKM 2018</div>
    </div>

    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">CUHK Postgraduate Student Scholarship</div>
     <div style="float:right;">2017-2021</div>
    </div>

    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">Best Student Paper Runner-up</div>
     <div style="float:right;">ACML 2016</div>
    </div>

<!--     <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">Meritorious Winner of the Mathmematical Contest in Modeling (MCM</div>
     <div style="float:right;">2016</div>
    </div> -->

    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">National Scholarship</div>
     <div style="float:right;">2015</div>
    </div>

    <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">Tang Lixin Scholarship</div>
     <div style="float:right;">2015</div>
    </div>

<H2>Working Experiences</H2>
<div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">Applied Scientist Intern at <A href="https://www.amazon.jobs/en/business_categories/amazon-devices">Amazon Devices</A></div>
     <div style="float:right;">2021 Summer</A></div>
    </div>
  
<div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">Research Intern at <A href="http://www.noahlab.com.hk/#/home">Huawei Noah's Ark Lab</A></div>
     <div style="float:right;">2020 Summer</A></div>
    </div>

<div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">Research Intern at <A href="https://ai.tencent.com/ailab/index.html">Tencent AI Lab</A></div>
     <div style="float:right;">2018 Summer</A></div>
    </div>

<!-- <div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">PhD. Candidate, <A href="http://www.cse.cuhk.edu.hk/en/">CSE Department</A>, <A href="http://www.cuhk.edu.hk/english/index.html">CUHK</A> </div>
     <div style="float:right;">2017-Now</A></div>
    </div>

<div style="overflow:hidden;margin-bottom: 0.6em">
     <div style="float:left;">B.Eng., <A href="http://www.yingcai.uestc.edu.cn/">Yingcai Honors College</A>, <A href="http://www.uestc.edu.cn/">UESTC</A></div>
     <div style="float:right;">2013-2017</div>
    </div> -->

<H2>Teaching Assistant</H2>
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;">CSCI3100: Software Engineering</div>
    <div style="float:right;">2020 Spring</div>
  </div>
   
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;">CSCI3100: Software Engineering</div>
    <div style="float:right;">2019 Spring</div>
  </div>
   
  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;">CSCI1540: Introduction to C++</div>
    <div style="float:right;">2018 Fall</div>
  </div>

  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;">CSCI3100: Software Engineering</div>
    <div style="float:right;">2018 Spring</div>
  </div>

<!--
<H2>External Reviews</H2>

  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;">ICSE, DSN, WWW, KDD, EDCC, ISSRE, NCA</div>
    <div style="float:right;">2016</div>
  </div>

  <div style="overflow:hidden;margin-bottom: 0.6em">
    <div style="float:left;">ICSE, WWW, NIPS, ISSRE, WSDM, ACML</div>
    <div style="float:right;">2015</div>
  </div>
-->

<div id="footer">
  <div id="footer-text"></div>
</div>
<!-- <a href="https://hits.seeyoufarm.com"><img src="https://hits.seeyoufarm.com/api/count/incr/badge.svg?url=https%3A%2F%2Fhaolibai.github.io&count_bg=%2379C83D&title_bg=%23555555&icon=&icon_color=%23E7E7E7&title=Hello&edge_flat=false"/></a> -->
  
<footer>
  &copy;
  Last modified on: 
  <script language="Javascript">
    var theDate = new Date(document.lastModified).toISOString().supstr(0,7)
    document.write(theDate);
  </SCRIPT>
  <a href="#top">To top <i class="icon-arrow-up"></i></a>

<p><center>
<div id="clustrmaps-widget" style="width:50%">
  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=nQLu84viePq1dfXIYnjoGiQxqm-r7hYJeX8f_Oug7fo&cl=ffffff&w=a"></script>
</div>        
</center></p>
</div>
</footer>    
    
<!-- jQuery (necessary for Bootstrap's JavaScript plugins) -->
<script src="js/jquery.min.js"></script>
<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="js/bootstrap.min.js"></script>
    
</DIV></DIV></BODY></HTML>
