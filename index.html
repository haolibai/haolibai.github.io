<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Haoli Bai (ÊüèÊòäÁ´ã)</title>
  
  <!-- Google Fonts -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=DM+Sans:ital,opsz,wght@0,9..40,400;0,9..40,500;0,9..40,600;0,9..40,700;1,9..40,400&family=Playfair+Display:wght@500;600;700&display=swap" rel="stylesheet">
  
  <!-- Main Stylesheet -->
  <link rel="stylesheet" href="style/main.css">
</head>
<body>
  <div class="container">
    
    <!-- ============================================
         Header Section
         ============================================ -->
    <header>
      <div class="header-content">
        <img src="img/square_light.jpg" alt="Haoli Bai" class="profile-image">
        <div class="header-info">
          <h1>Haoli Bai (ÊüèÊòäÁ´ã)</h1>
          <p class="affiliation">Researcher, Huawei Hong Kong Research Center</p>
          <p class="location">Hong Kong SAR, China</p>
          <p class="contact-email"><strong>Email:</strong> haolibai [at] gmail.com</p>
          <div class="social-links">
            <a href="https://scholar.google.com/citations?user=pk7jX3gAAAAJ&hl=zh-CN" title="Google Scholar">
              <img src="img/google_scholar.png" alt="Google Scholar">
            </a>
            <a href="https://dblp.org/pid/195/9712.html" title="DBLP">
              <img src="img/dblp.png" alt="DBLP">
            </a>
            <a href="https://github.com/haolibai" title="GitHub">
              <img src="img/github_s.jpg" alt="GitHub">
            </a>
            <a href="https://www.linkedin.com/in/baihaoli-407136142/" title="LinkedIn">
              <img src="img/LinkedIn_s.png" alt="LinkedIn">
            </a>
          </div>
        </div>
      </div>
    </header>

    <!-- ============================================
         Navigation
         ============================================ -->
    <nav>
      <div class="container">
        <ul>
          <li><a href="#about">About</a></li>
          <li><a href="#news">News</a></li>
          <li><a href="#publications">Publications</a></li>
          <li><a href="#talks">Talks</a></li>
          <li><a href="#projects">Projects</a></li>
          <li><a href="#services">Services</a></li>
          <li><a href="#awards">Awards</a></li>
          <li><a href="#experience">Experience</a></li>
        </ul>
      </div>
    </nav>

    <!-- ============================================
         Main Content
         ============================================ -->
    <main>
      
      <!-- About Section -->
      <section id="about">
        <h2>About</h2>
        <div class="bio">
          <p>I am currently a researcher at the Language Model Lab, Huawei Hong Kong Research Center. I obtained my Ph.D. degree from The Chinese University of Hong Kong supervised by <a href="https://www.cse.cuhk.edu.hk/lyu/">Prof. Michael R. Lyu</a> and <a href="https://www.cse.cuhk.edu.hk/irwin.king/">Prof. Irwin King</a>, and the B.Eng. Degree from Yingcai Honors College of University of Electronic Science and Technology.</p>
          <p>Our team's effort is on large language models with topics spanning from pre-training, post-training, to agentic AI (e.g., deep research and coding agent). I am also an experienced researcher in LLM efficiency, e.g., compression and acceleration of LLMs.</p>
          <div class="hiring-notice">
            <span class="fire">üî•</span>
            <span><strong>[Hiring]</strong> We are constantly looking for full-time researchers and research interns with solid algorithm or system background (Base: HK or Shenzhen). Please connect by E-Mail.</span>
          </div>
        </div>
      </section>

      <!-- News Section -->
      <section id="news">
        <h2>News</h2>
        <ul class="news-list">
          <li class="news-item">
            <span class="news-date">2026-1</span>
            <span class="news-content">üî• We present <a href="https://arxiv.org/abs/2601.01426"> SWE-Lego </a>, the state-of-the-art supervised fine-tuning method for software issue resolving. All code, data, models are now opensourced. <a href="https://swe-lego.github.io/"> Project website</a>.</span>
          </li>
          <li class="news-item">
            <span class="news-date">2025-11</span>
            <span class="news-content">We will present the tutorial "Efficient Inference for Large Language Models ‚Äì Algorithm, Model, and System" at EMNLP 2025. <a href="https://haolibai.github.io/emnlp-2025-tutorial-efficiency/">Tutorial website.</a></span>
          </li>
          <li class="news-item">
            <span class="news-date">2025-11</span>
            <span class="news-content">I will give a talk on "Quantization and Pruning of Large Language Models: Challenges, Techniques and Opportunities" at <a href="http://lmg.cipsc.org.cn/conference/lmg2025/subForum/subForum4/index.html">LMG 2025</a>.</span>
          </li>
          <li class="news-item">
            <span class="news-date">2025-9</span>
            <span class="news-content">Our paper <a href="https://arxiv.org/abs/2505.24680">"A Simple Linear Patch Revives Layer-Pruned Large Language Models"</a> is accepted by NeurIPS 2025.</span>
          </li>
          <!-- <li class="news-item">
            <span class="news-date">2025-7</span>
            <span class="news-content">üî• Our paper <a href="https://arxiv.org/abs/2504.04823v1">"Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models"</a> is accepted by COLM 2025. It was on the top-10 trending list on <a href="https://www.alphaxiv.org/abs/2504.04823">alphaXiv</a>.</span>
          </li> -->
          <!-- <li class="news-item">
            <span class="news-date">2025-4</span>
            <span class="news-content">I will serve as the Area Chair for NeurIPS 2025.</span>
          </li> -->
        </ul>
      </section>

      <!-- Publications Section -->
      <section id="publications">
        <h2>Selected Publications</h2>
        <p class="pub-note">*: Equal contribution; #: Corresponding author; +: Project lead</p>

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2601.01426">SWE-Lego: Pushing the Limits of Supervised Fine-tuning for Software Issue Resolving</a>
          </div>
          <div class="pub-authors">Chaofan Tao<sup>*</sup>, Jierun Chen<sup>*</sup>, Yuxin Jiang<sup>*</sup>, Kaiqi Kou<sup>*</sup>, Shaowei Wang<sup>*</sup>, Ruoyu Wang<sup>*</sup>, Xiaohui Li<sup>#</sup>, Sidi Yang, Yiming Du, Jianbo Dai, Zhiming Mao, Xinyu Wang, Lifeng Shang, <strong>Haoli Bai</strong><sup>#</sup></div>
          <div class="pub-venue">arXiv Preprint 2601.01426.</div>
          <div class="pub-links">
            <a href="https://swe-lego.github.io/">Homepage</a> <a href="https://github.com/SWE-Lego/SWE-Lego">Code</a> <a href="https://huggingface.co/SWE-Lego">Data & Models</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2505.24680">A Simple Linear Patch Revives Layer-Pruned Large Language Models</a>
          </div>
          <div class="pub-authors">Xinrui Chen, <strong>Haoli Bai</strong><sup>#+</sup>, Tao Yuan, Ruikang Liu, Kang Zhao, Xianzhi Yu, Lu Hou, Tian Guan, Yonghong He, Chun Yuan<sup>#</sup></div>
          <div class="pub-venue">Proceedings of the 39th conference on Neural Information Processing Systems (NeurIPS), 2025.</div>
          <div class="pub-links">
            <a href="https://github.com/XinruiChen/Simple-Linear-Patch">Code</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2504.04823v1">Quantization Hurts Reasoning? An Empirical Study on Quantized Reasoning Models</a>
          </div>
          <div class="pub-authors">Ruikang Liu<sup>*</sup>, Yuxuan Sun<sup>*</sup>, Manyi Zhang<sup>*</sup>, <strong>Haoli Bai</strong><sup>#+</sup>, Xianzhi Yu, Tiezheng Yu, Chun Yuan, Lu Hou<sup>#</sup></div>
          <div class="pub-venue">Conference on Language Modeling (COLM), 2025.</div>
          <div class="pub-links">
            <a href="https://github.com/ruikangliu/Quantized-Reasoning-Models">Code</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/pdf/2410.09426">FlatQuant: Flatness Matters for LLM Quantization</a>
          </div>
          <div class="pub-authors">Yuxuan Sun<sup>*</sup>, Ruikang Liu<sup>*</sup>, <strong>Haoli Bai</strong><sup>#+</sup>, Han Bao, Kang Zhao, Yuening Li, Jiaxin Hu, Xianzhi Yu, Lu Hou, Chun Yuan, Xin Jiang, Wulong Liu, Jun Yao</div>
          <div class="pub-venue">International Conference on Machine Learning (ICML), 2025.</div>
          <div class="pub-links">
            <a href="https://github.com/ruikangliu/FlatQuant">Code</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://aclanthology.org/2024.naacl-long.264.pdf">Visually Guided Generative Text-Layout Pre-training for Document Intelligence</a>
          </div>
          <div class="pub-authors">Zhiming Mao, <strong>Haoli Bai</strong><sup>#+</sup>, Lu Hou, Lifeng Shang, Xin Jiang, Qun Liu, Kam-Fai Wong</div>
          <div class="pub-venue">The North American Chapter of the Association for Computational Linguistics (NAACL), 2024.</div>
          <div class="pub-links">
            <a href="https://github.com/Veason-silverbullet/ViTLP">Code</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2403.01241">IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact</a>
          </div>
          <div class="pub-authors">Ruikang Liu, <strong>Haoli Bai</strong><sup>+</sup>, Haokun Lin, Yuening Li, Han Gao, Zhengzhuo Xu, Lu Hou, Jun Yao, Chun Yuan</div>
          <div class="pub-venue">Findings of Annual Meeting of the Association for Computational Linguistics (ACL), 2024.</div>
          <div class="pub-links">
            <a href="https://github.com/ruikangliu/IntactKV">Code</a>
          </div>
        </div>

        <!-- <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2403.07839v1">MoPE-CLIP: Structured Pruning for Efficient Vision-Language Models with Module-wise Pruning Error Metric</a>
          </div>
          <div class="pub-authors">Haokun Lin, <strong>Haoli Bai</strong><sup>+</sup>, Zhili Liu, Lu Hou, Muyi Sun, Linqi Song, Ying Wei, Zhenan Sun</div>
          <div class="pub-venue">Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.</div>
        </div> -->

        <div class="publication">
          <div class="pub-title">
            <a href="https://openreview.net/forum?id=Tr0lPx9woF">Plug-and-Play: An Efficient Post-training Pruning Method for Large Language Models</a>
          </div>
          <div class="pub-authors">Yingtao Zhang, <strong>Haoli Bai</strong><sup>+</sup>, Haokun Lin, Jialin Zhao, Lu Hou, Carlo Vittorio Cannistraci</div>
          <div class="pub-venue">The Twelfth International Conference on Learning Representations (ICLR), 2024.</div>
          <div class="pub-links">
            <a href="https://github.com/biomedical-cybernetics/Relative-importance-and-activation-pruning">Code</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2212.09621">Wukong-Reader: Multi-modal Pre-training for Fine-grained Visual Document Understanding</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong><sup>*</sup>, Zhiguang Liu<sup>*</sup>, Xiaojun Meng<sup>*</sup>, Wentao Li, Shuang Liu, Nian Xie, Rongfu Zheng, Liangwei Wang, Lu Hou, Jiansheng Wei, Xin Jiang, Qun Liu</div>
          <div class="pub-venue">The 61th Annual Meeting of the Association for Computational Linguistics (ACL), 2023.</div>
        </div>

        <!-- <div class="publication">
          <div class="pub-title">
            <span>Structured Pruning for Efficient Generative Pre-trained Language Models</span>
          </div>
          <div class="pub-authors">Chaofan Tao, Lu Hou<sup>+</sup>, <strong>Haoli Bai</strong><sup>+</sup>, Jiansheng Wei, Xin Jiang, Qun Liu, Ping Luo, Ngai Wong</div>
          <div class="pub-venue">Findings of The 61th Annual Meeting of the Association for Computational Linguistics (ACL), 2023.</div>
        </div> -->

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/pdf/2109.15082.pdf">Towards Efficient Post-training Quantization of Pre-trained Language Models</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong>, Lu Hou, Lifeng Shang, Xin Jiang, Irwin King, Michael Lyu</div>
          <div class="pub-venue">Proceedings of the 36th conference on Neural Information Processing Systems (NeurIPS), 2022.</div>
        </div>

        <!-- <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2111.09499">Dynamically pruning segformer for efficient semantic segmentation</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong>, Hongda Mao, Dinesh Nair</div>
          <div class="pub-venue">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2022.</div>
        </div> -->

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/2012.15701">BinaryBERT: Pushing the Limit of BERT Quantization</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong>, Wei Zhang, Lu Hou, Lifeng Shang, Jing Jin, Xin Jiang, Qun Liu, Michael Lyu, Irwin King</div>
          <div class="pub-venue">The 59th Annual Meeting of the Association for Computational Linguistics (ACL), 2021. <strong>Accepted with scores 5, 5, 4.</strong></div>
          <div class="pub-links">
            <a href="https://github.com/huawei-noah/Pretrained-Language-Model/tree/master/BinaryBERT">Code</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://proceedings.neurips.cc/paper/2020/file/42cd63cb189c30ed03e42ce2c069566c-Paper.pdf">Revisiting Parameter Sharing for Automatic Neural Channel Number Search</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong><sup>*</sup>, Jiaxing Wang<sup>*</sup>, Jiaxiang Wu, Xupeng Shi, Junzhou Huang, Irwin King, Michael Lyu, Jian Cheng</div>
          <div class="pub-venue">Proceedings of the 34th conference on Neural Information Processing Systems (NeurIPS), 2020.</div>
          <div class="pub-links">
            <a href="https://github.com/haolibai/APS-channel-search">Code</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://arxiv.org/abs/1911.09450">Few Shot Network Compression via Cross Distillation</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong>, Jiaxiang Wu, Irwin King, Michael Lyu</div>
          <div class="pub-venue">Proceedings of the 34th AAAI Conference on Artificial Intelligence (AAAI), 2020.</div>
          <div class="pub-links">
            <a href="https://github.com/haolibai/Cross-Distillation">Code</a>
            <a href="assets/aaai20_poster.pdf">Poster</a>
          </div>
        </div>

        <div class="publication">
          <div class="pub-title">
            <a href="https://17a11ed1-a-62cb3a1a-s-sites.googlegroups.com/site/nipsts2017/NIPS_2017_TSW_paper_19.pdf">Neural Relational Topic Models for Scientific Article Analysis</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong>, Zhuangbin Chen, Michael Lyu, Irwin King, Zenglin Xu</div>
          <div class="pub-venue">Proceedings of The 27th International Conference on Information and Knowledge Management (CIKM), 2018.</div>
          <div class="pub-links">
            <a href="https://github.com/zbchern/Neural-Relational-Topic-Models">Code</a>
          </div>
        </div>

        <!-- <div class="publication">
          <div class="pub-title">
            <a href="http://proceedings.mlr.press/v63/bai103.html">Hierarchical Probabilistic Matrix Factorization with Network Topology for Multi-relational Social Network</a>
          </div>
          <div class="pub-authors"><strong>Haoli Bai</strong>, Zenglin Xu, Bin Liu, Yingming Li</div>
          <div class="pub-venue">Proceedings of The 8th Asian Conference on Machine Learning (ACML), 2016. <strong>Best Student Paper Runner-up</strong>.</div>
        </div> -->
      </section>

      <!-- Talks Section -->
      <section id="talks">
        <h2>Invited Talks</h2>
        <ul class="talks-list">
          <li class="talk-item">
            "Quantization and Pruning of Large Language Models: Challenges, Techniques and Opportunities" at <a href="https://www.slai.edu.cn/">SLAI</a>, 2025. <a href="assets/quantization-pruning-public.pdf">[Slide]</a>
          </li>
          <li class="talk-item">
            "Efficient Inference for Large Language Models ‚Äì Algorithm, Model, and System" at EMNLP Tutorial, 2025. <a href="https://haolibai.github.io/emnlp-2025-tutorial-efficiency/">[Tutorial website]</a>
          </li>
          <li class="talk-item">
            "Quantization and Pruning of Large Language Models: Challenges, Techniques and Opportunities" at <a href="http://lmg.cipsc.org.cn/conference/lmg2025/subForum/subForum4/index.html">LMG</a>, 2025.
          </li>
        </ul>
      </section>

      <!-- Projects Section -->
      <section id="projects">
        <h2>Projects</h2>
        <div class="project">
          <div class="project-header">
            <span class="project-title">PocketFlow: An Automated Framework for Compressing and Accelerating DNNs</span>
            <div class="project-links">
              <a href="https://github.com/tencent/PocketFlow">Code</a>
              <a href="https://pocketflow.github.io/">Doc</a>
            </div>
          </div>
          <p class="project-desc">PocketFlow automatically searches for optimal model compression strategies such as network pruning, quantization, knowledge distillation with little human efforts, and also supports TFLite deployment on Android devices. It has collected 2600+ stars and 480+ forks.</p>
        </div>
      </section>

      <!-- Services Section -->
      <section id="services">
        <h2>Services</h2>
        <div class="service-category">
          <div class="service-title">Area Chair</div>
          <div class="service-content">NeurIPS 2025</div>
        </div>
        <div class="service-category">
          <div class="service-title">Senior PC Member</div>
          <div class="service-content">IJCAI 2021</div>
        </div>
        <div class="service-category">
          <div class="service-title">PC Member</div>
          <div class="service-content">ICLR 22-25, ICML 21-25, NeurIPS 20-24, ACL ARR 25, COLM 25, ICCV 25, AAAI 19-21, IJCAI 20</div>
        </div>
        <div class="service-category">
          <div class="service-title">Journal Reviewer</div>
          <div class="service-content">T-PAMI, Neural Networks, etc.</div>
        </div>
      </section>

      <!-- Awards Section -->
      <section id="awards">
        <h2>Selected Awards</h2>
        <div class="timeline-item">
          <span class="timeline-title">Excellent Intern, Huawei Noah's Ark Lab</span>
          <span class="timeline-year">2021</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">AAAI Student Travel Grant</span>
          <span class="timeline-year">2020</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">ACM Student Travel Grant, CIKM</span>
          <span class="timeline-year">2018</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">CUHK Postgraduate Student Scholarship</span>
          <span class="timeline-year">2017-2021</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">Best Student Paper Runner-up, ACML</span>
          <span class="timeline-year">2016</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">National Scholarship</span>
          <span class="timeline-year">2015</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">Tang Lixin Scholarship</span>
          <span class="timeline-year">2015</span>
        </div>
      </section>

      <!-- Experience Section -->
      <section id="experience">
        <h2>Experience</h2>
        
        <h3>Work Experience</h3>
        <div class="timeline-item">
          <span class="timeline-title">Applied Scientist Intern at <a href="https://www.amazon.jobs/en/business_categories/amazon-devices">Amazon Devices</a></span>
          <span class="timeline-year">2021 Summer</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">Research Intern at <a href="http://www.noahlab.com.hk/#/home">Huawei Noah's Ark Lab</a></span>
          <span class="timeline-year">2020 Summer</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">Research Intern at <a href="https://ai.tencent.com/ailab/index.html">Tencent AI Lab</a></span>
          <span class="timeline-year">2018 Summer</span>
        </div>

        <h3>Teaching Assistant</h3>
        <div class="timeline-item">
          <span class="timeline-title">CSCI3100: Software Engineering</span>
          <span class="timeline-year">2020 Spring</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">CSCI3100: Software Engineering</span>
          <span class="timeline-year">2019 Spring</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">CSCI1540: Introduction to C++</span>
          <span class="timeline-year">2018 Fall</span>
        </div>
        <div class="timeline-item">
          <span class="timeline-title">CSCI3100: Software Engineering</span>
          <span class="timeline-year">2018 Spring</span>
        </div>
      </section>

    </main>

    <!-- ============================================
         Footer
         ============================================ -->
    <footer>
      <div class="footer-content">
        <p>&copy; <span id="year"></span> Haoli Bai ¬∑ <a href="#" id="top-link">Back to top ‚Üë</a></p>
      </div>
      <div class="visitor-map">
        <div id="clustrmaps-widget">
          <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=nQLu84viePq1dfXIYnjoGiQxqm-r7hYJeX8f_Oug7fo&cl=ffffff&w=a"></script>
        </div>
      </div>
    </footer>

  </div>

  <!-- ============================================
       JavaScript
       ============================================ -->
  <script>
    // Set current year
    document.getElementById('year').textContent = new Date().getFullYear();

    // Smooth scroll for navigation
    document.querySelectorAll('nav a, #top-link').forEach(link => {
      link.addEventListener('click', function(e) {
        if (this.id === 'top-link') {
          e.preventDefault();
          window.scrollTo({ top: 0, behavior: 'smooth' });
          return;
        }
        const href = this.getAttribute('href');
        if (href.startsWith('#')) {
          e.preventDefault();
          const target = document.querySelector(href);
          if (target) {
            target.scrollIntoView({ behavior: 'smooth', block: 'start' });
          }
        }
      });
    });

    // Add scroll-based nav highlight
    const sections = document.querySelectorAll('section[id]');
    const navLinks = document.querySelectorAll('nav a');

    function highlightNav() {
      let current = '';
      sections.forEach(section => {
        const sectionTop = section.offsetTop - 100;
        if (pageYOffset >= sectionTop) {
          current = section.getAttribute('id');
        }
      });

      navLinks.forEach(link => {
        link.classList.remove('active');
        if (link.getAttribute('href') === `#${current}`) {
          link.classList.add('active');
        }
      });
    }

    window.addEventListener('scroll', highlightNav);
    highlightNav();
  </script>
</body>
</html>

